{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP+CSS 201 - Fall 2021 - Controlling for Text.ipynb",
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c53f73b36ed04c048be4a7519ee41c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d6c979b1b864864a91e7f7300e35300",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_75c6bc8a639748e8af7ea0460360dcd9",
              "IPY_MODEL_2d706461ff9d41d4a0665cc5a69a2516",
              "IPY_MODEL_64208d402a4a40c6ad1496a3d4166e73"
            ]
          }
        },
        "2d6c979b1b864864a91e7f7300e35300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75c6bc8a639748e8af7ea0460360dcd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f8edabc9ac24f6780b5d27e892de184",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f57dc22543664280b4fb95192c28d323"
          }
        },
        "2d706461ff9d41d4a0665cc5a69a2516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cef009cce94f4c32876938463a6d563d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9816,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9816,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34df0bcd87f249f982cf3a902cb886f2"
          }
        },
        "64208d402a4a40c6ad1496a3d4166e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e544d18b5c974057b49b111b5149494f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9816/9816 [00:03&lt;00:00, 2863.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3a811b295ad45b5b4beef98095c5d67"
          }
        },
        "6f8edabc9ac24f6780b5d27e892de184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f57dc22543664280b4fb95192c28d323": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cef009cce94f4c32876938463a6d563d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34df0bcd87f249f982cf3a902cb886f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e544d18b5c974057b49b111b5149494f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3a811b295ad45b5b4beef98095c5d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1d3JJcjsTi4"
      },
      "source": [
        "<a href=\"http://data.bus.wisc.edu/\">\n",
        "        <img align=\"right\" src=\"https://emaadmanzoor.com/images/color-UWcrest-print.png\" height=80 style=\"height: 80px; float: right;\"/>\n",
        "</a>\n",
        "\n",
        "# NLP+CSS 201: Controlling for Text in Causal<br/>Inference with Double Machine Learning\n",
        "\n",
        "*By [Emaad Manzoor](http://emaadmanzoor.com), [Wisconsin School of Business](https://business.wisc.edu/)*\n",
        "\n",
        "**The tutorial slides and video can be found [here](https://nlp-css-201-tutorials.github.io/nlp-css-201-tutorials/).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQCmLeh9t8cC"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWtrMZuuDpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f625f9-6dfa-43c6-898c-0e8075008c9c"
      },
      "source": [
        "!pip install doubleml==0.3.0\n",
        "!pip install sklearn\n",
        "!pip install statsmodels==0.10.1\n",
        "!pip install econml==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting doubleml==0.3.0\n",
            "  Downloading DoubleML-0.3.0-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from doubleml==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from doubleml==0.3.0) (0.10.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from doubleml==0.3.0) (1.1.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from doubleml==0.3.0) (0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from doubleml==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from doubleml==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->doubleml==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->doubleml==0.3.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->doubleml==0.3.0) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->doubleml==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->doubleml==0.3.0) (0.5.2)\n",
            "Installing collected packages: doubleml\n",
            "Successfully installed doubleml-0.3.0\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Collecting statsmodels==0.10.1\n",
            "  Downloading statsmodels-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (8.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.10.1) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.10.1) (1.1.5)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.10.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.10.1) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels==0.10.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels==0.10.1) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.0->statsmodels==0.10.1) (1.15.0)\n",
            "Installing collected packages: statsmodels\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed statsmodels-0.10.1\n",
            "Collecting econml==0.6.0\n",
            "  Downloading econml-0.6-py3-none-any.whl (277 kB)\n",
            "\u001b[K     |████████████████████████████████| 277 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (2.7.0)\n",
            "Collecting tensorflow==1.*\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 396 bytes/s \n",
            "\u001b[?25hRequirement already satisfied: numba!=0.42.1 in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (0.51.2)\n",
            "Collecting scikit-learn~=0.21.0\n",
            "  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (0.10.1)\n",
            "Collecting matplotlib<3.1\n",
            "  Downloading matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl (13.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 90 kB/s \n",
            "\u001b[?25hCollecting sparse\n",
            "  Downloading sparse-0.13.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: statsmodels>=0.9 in /usr/local/lib/python3.7/dist-packages (from econml==0.6.0) (0.10.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (0.8.1)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 103.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (1.1.2)\n",
            "Collecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (1.41.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (0.12.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 49.5 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.*->econml==0.6.0) (3.17.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.1->econml==0.6.0) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.1->econml==0.6.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.1->econml==0.6.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.1->econml==0.6.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba!=0.42.1->econml==0.6.0) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba!=0.42.1->econml==0.6.0) (0.34.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.9->econml==0.6.0) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.9->econml==0.6.0) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19->statsmodels>=0.9->econml==0.6.0) (2018.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.*->econml==0.6.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.*->econml==0.6.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.*->econml==0.6.0) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.*->econml==0.6.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.*->econml==0.6.0) (3.10.0.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=cbd363544bd5f9bab61336fed90355857cf2c0a0df50852e218b0d4badb5b8c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow, sparse, scikit-learn, matplotlib, econml\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQP9W7X7sTi9"
      },
      "source": [
        "#import doubleml\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "#from econml.dml import LinearDMLCateEstimator\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_XSVAlrsTi-"
      },
      "source": [
        "## Fetch and relabel 20Newsgroups data, construct TF-IDF matrices\n",
        "\n",
        "Experiment with the TfidfVectorizer parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVlcYRljsTi_",
        "outputId": "b48b31e2-da60-4839-dd56-fd194502c84b"
      },
      "source": [
        "newsgroup_data = fetch_20newsgroups(subset=\"train\", remove=[\"headers\", \"footers\", \"quotes\"], shuffle=False)\n",
        "\n",
        "target_name_to_label = {\n",
        " 'comp.graphics': 1,\n",
        " 'comp.os.ms-windows.misc': 1,\n",
        " 'comp.sys.ibm.pc.hardware': 1,\n",
        " 'comp.sys.mac.hardware': 1,\n",
        " 'comp.windows.x': 1,\n",
        " 'misc.forsale': 2,\n",
        " 'rec.autos': 3,\n",
        " 'rec.motorcycles': 3,\n",
        " 'rec.sport.baseball': 4,\n",
        " 'rec.sport.hockey': 4,\n",
        " 'sci.crypt': 5,\n",
        " 'sci.electronics': 6,\n",
        " 'sci.med': 7,\n",
        " 'sci.space': 8,\n",
        " 'alt.atheism': 9,\n",
        " 'soc.religion.christian': 9,\n",
        " 'talk.religion.misc': 9, \n",
        " 'talk.politics.guns': 10,\n",
        " 'talk.politics.mideast': 10,\n",
        " 'talk.politics.misc': 10,\n",
        "}\n",
        "label_names = [\"comp\", \"sale\", \"auto\", \"sport\", \"crypt\", \"electronics\", \"med\", \"space\", \"religion\", \"politics\"]\n",
        "labels = [target_name_to_label[newsgroup_data.target_names[original_label]]\n",
        "              for original_label in newsgroup_data.target]\n",
        "labels = np.array(labels)\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=25, max_df=0.01)\n",
        "tfidf_vectors = vectorizer.fit_transform(newsgroup_data.data).toarray()\n",
        "print(tfidf_vectors.shape, labels.shape)\n",
        "\n",
        "documents = []\n",
        "dropped_document_idx = set([])\n",
        "vocabset = set(vectorizer.get_feature_names())\n",
        "for idx, text in enumerate(newsgroup_data.data):\n",
        "    document = []\n",
        "    for token in text.split(\" \"):\n",
        "        if token not in vocabset:\n",
        "            continue\n",
        "        document.append(token)\n",
        "\n",
        "    if len(document) == 0:\n",
        "        dropped_document_idx.add(idx)\n",
        "        continue\n",
        "    \n",
        "    document = \" \".join(document)\n",
        "    if len(document) > 1000:\n",
        "        document = document[:1000]\n",
        "\n",
        "    documents.append(document)\n",
        "\n",
        "tfidf_vectors = np.array([tfidf_vectors[idx, :]\n",
        "                          for idx in range(tfidf_vectors.shape[0])\n",
        "                          if idx not in dropped_document_idx])\n",
        "labels = np.array([labels[idx]\n",
        "                   for idx in range(len(labels))\n",
        "                   if idx not in dropped_document_idx])\n",
        "print(tfidf_vectors.shape, labels.shape, len(documents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11314, 3882) (11314,)\n",
            "(9816, 3882) (9816,) 9816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi5f_inRsTjB"
      },
      "source": [
        "## Simulate Data for Treatment Effects\n",
        "\n",
        "For each document $i$, a row of the data consists of:\n",
        "\n",
        "   * Unobserved confounder $U_i$: Binary, equals 1 if text is from a newsgroup on religion\n",
        "   * Treatment $Z_i$: depends on $U_i$\n",
        "   * Outcome $Y_i$: Real-valued, depends on $Z_i$ and $U_i$\n",
        "   * Document TF-IDF vector $\\pmb{X}_i$\n",
        "   \n",
        "**The true treatment effect is 0.05.**\n",
        "\n",
        "You can play around with the manner of simulation (eg. increase the treatment effect size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c53f73b36ed04c048be4a7519ee41c08",
            "2d6c979b1b864864a91e7f7300e35300",
            "75c6bc8a639748e8af7ea0460360dcd9",
            "2d706461ff9d41d4a0665cc5a69a2516",
            "64208d402a4a40c6ad1496a3d4166e73",
            "6f8edabc9ac24f6780b5d27e892de184",
            "f57dc22543664280b4fb95192c28d323",
            "cef009cce94f4c32876938463a6d563d",
            "34df0bcd87f249f982cf3a902cb886f2",
            "e544d18b5c974057b49b111b5149494f",
            "e3a811b295ad45b5b4beef98095c5d67"
          ]
        },
        "id": "UkA1JfY4sTjB",
        "outputId": "a82bc4f0-9ad1-423f-a019-d581c5319db3"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1. / (1. + math.exp(-x))\n",
        "\n",
        "simulated_data = []\n",
        "for idx in tqdm(range(tfidf_vectors.shape[0])):    \n",
        "    # confounder\n",
        "    U = int(labels[idx]==9) # unobserved confounder = religion topic, influences text\n",
        "        \n",
        "    # treatment\n",
        "    Z = 2.*U + np.random.normal(loc=0.5, scale=5.0)  # Z depends on U\n",
        "    \n",
        "    # binary outcome\n",
        "    # yprob = sigmoid(-0.5 + 0.05*Z + 5.0*U)\n",
        "    # Y = np.random.choice(a=[0, 1],  p=[1.0 - yprob, yprob])\n",
        "    Y = -0.5 + 0.05*Z + 5.0*U + np.random.normal(0.0, 0.1)\n",
        "    \n",
        "    simulated_data.append([Y, Z, U] + list(tfidf_vectors[idx]))\n",
        "\n",
        "simulated_data = np.array(simulated_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c53f73b36ed04c048be4a7519ee41c08",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/9816 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SIymvlDsTjC"
      },
      "source": [
        "## Convert to Pandas Dataframe, Describe Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "bPTlSRHzsTjC",
        "outputId": "a8de20a4-e188-4172-a3ce-68eabb9c70fa"
      },
      "source": [
        "df = pd.DataFrame(simulated_data)\n",
        "df.columns = [\"Y\", \"Z\", \"U\"] + [\"Word\" + str(i) for i in range(tfidf_vectors.shape[1])]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>U</th>\n",
              "      <th>Word0</th>\n",
              "      <th>Word1</th>\n",
              "      <th>Word2</th>\n",
              "      <th>Word3</th>\n",
              "      <th>Word4</th>\n",
              "      <th>Word5</th>\n",
              "      <th>Word6</th>\n",
              "      <th>Word7</th>\n",
              "      <th>Word8</th>\n",
              "      <th>Word9</th>\n",
              "      <th>Word10</th>\n",
              "      <th>Word11</th>\n",
              "      <th>Word12</th>\n",
              "      <th>Word13</th>\n",
              "      <th>Word14</th>\n",
              "      <th>Word15</th>\n",
              "      <th>Word16</th>\n",
              "      <th>Word17</th>\n",
              "      <th>Word18</th>\n",
              "      <th>Word19</th>\n",
              "      <th>Word20</th>\n",
              "      <th>Word21</th>\n",
              "      <th>Word22</th>\n",
              "      <th>Word23</th>\n",
              "      <th>Word24</th>\n",
              "      <th>Word25</th>\n",
              "      <th>Word26</th>\n",
              "      <th>Word27</th>\n",
              "      <th>Word28</th>\n",
              "      <th>Word29</th>\n",
              "      <th>Word30</th>\n",
              "      <th>Word31</th>\n",
              "      <th>Word32</th>\n",
              "      <th>Word33</th>\n",
              "      <th>Word34</th>\n",
              "      <th>Word35</th>\n",
              "      <th>Word36</th>\n",
              "      <th>...</th>\n",
              "      <th>Word3842</th>\n",
              "      <th>Word3843</th>\n",
              "      <th>Word3844</th>\n",
              "      <th>Word3845</th>\n",
              "      <th>Word3846</th>\n",
              "      <th>Word3847</th>\n",
              "      <th>Word3848</th>\n",
              "      <th>Word3849</th>\n",
              "      <th>Word3850</th>\n",
              "      <th>Word3851</th>\n",
              "      <th>Word3852</th>\n",
              "      <th>Word3853</th>\n",
              "      <th>Word3854</th>\n",
              "      <th>Word3855</th>\n",
              "      <th>Word3856</th>\n",
              "      <th>Word3857</th>\n",
              "      <th>Word3858</th>\n",
              "      <th>Word3859</th>\n",
              "      <th>Word3860</th>\n",
              "      <th>Word3861</th>\n",
              "      <th>Word3862</th>\n",
              "      <th>Word3863</th>\n",
              "      <th>Word3864</th>\n",
              "      <th>Word3865</th>\n",
              "      <th>Word3866</th>\n",
              "      <th>Word3867</th>\n",
              "      <th>Word3868</th>\n",
              "      <th>Word3869</th>\n",
              "      <th>Word3870</th>\n",
              "      <th>Word3871</th>\n",
              "      <th>Word3872</th>\n",
              "      <th>Word3873</th>\n",
              "      <th>Word3874</th>\n",
              "      <th>Word3875</th>\n",
              "      <th>Word3876</th>\n",
              "      <th>Word3877</th>\n",
              "      <th>Word3878</th>\n",
              "      <th>Word3879</th>\n",
              "      <th>Word3880</th>\n",
              "      <th>Word3881</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.183011</td>\n",
              "      <td>11.756561</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.085405</td>\n",
              "      <td>8.609010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.148449</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.102486</td>\n",
              "      <td>10.239682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024428</td>\n",
              "      <td>0.025255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050249</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021736</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024428</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021589</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.066852</td>\n",
              "      <td>9.093971</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.539129</td>\n",
              "      <td>1.899150</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9811</th>\n",
              "      <td>-0.434235</td>\n",
              "      <td>-0.025587</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9812</th>\n",
              "      <td>-0.254999</td>\n",
              "      <td>8.813957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9813</th>\n",
              "      <td>-0.780463</td>\n",
              "      <td>-4.592288</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9814</th>\n",
              "      <td>-0.341115</td>\n",
              "      <td>0.988236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9815</th>\n",
              "      <td>-0.631172</td>\n",
              "      <td>-2.338630</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9816 rows × 3885 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Y          Z    U  Word0  ...  Word3878  Word3879  Word3880  Word3881\n",
              "0     0.183011  11.756561  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "1    -0.085405   8.609010  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "2     0.102486  10.239682  0.0    0.0  ...       0.0       0.0  0.021589       0.0\n",
              "3    -0.066852   9.093971  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "4     4.539129   1.899150  1.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "...        ...        ...  ...    ...  ...       ...       ...       ...       ...\n",
              "9811 -0.434235  -0.025587  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "9812 -0.254999   8.813957  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "9813 -0.780463  -4.592288  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "9814 -0.341115   0.988236  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "9815 -0.631172  -2.338630  0.0    0.0  ...       0.0       0.0  0.000000       0.0\n",
              "\n",
              "[9816 rows x 3885 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "JjX30VOAsTjD",
        "outputId": "4c9b05ea-6772-487f-a1fd-faf9606b85e0"
      },
      "source": [
        "df[[\"Y\", \"Z\", \"U\"]].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>U</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9816.000000</td>\n",
              "      <td>9816.000000</td>\n",
              "      <td>9816.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.198048</td>\n",
              "      <td>0.702752</td>\n",
              "      <td>0.132641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.750072</td>\n",
              "      <td>5.038643</td>\n",
              "      <td>0.339203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.490108</td>\n",
              "      <td>-17.178154</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.633092</td>\n",
              "      <td>-2.739312</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.426790</td>\n",
              "      <td>0.663566</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-0.175845</td>\n",
              "      <td>4.163009</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.439211</td>\n",
              "      <td>20.845121</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Y            Z            U\n",
              "count  9816.000000  9816.000000  9816.000000\n",
              "mean      0.198048     0.702752     0.132641\n",
              "std       1.750072     5.038643     0.339203\n",
              "min      -1.490108   -17.178154     0.000000\n",
              "25%      -0.633092    -2.739312     0.000000\n",
              "50%      -0.426790     0.663566     0.000000\n",
              "75%      -0.175845     4.163009     0.000000\n",
              "max       5.439211    20.845121     1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3BpszwsTjE"
      },
      "source": [
        "## Treatment Effect Estimation 1: Regress $Y_i$ on $Z_i$ and $U_i$\n",
        "\n",
        "Since the estimation includes the unobserved confounder $U_i$, the treatment effect can\n",
        "be estimated without bias (there are no unobserved confounders)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLWkivgKsTjE",
        "outputId": "69ff39e5-4af2-4998-ab09-f405f9cce277"
      },
      "source": [
        "%%time\n",
        "y = simulated_data[:, 0]\n",
        "X = sm.add_constant(simulated_data[:, 1:3])\n",
        "model = sm.OLS(endog=y, exog=X)\n",
        "res = model.fit(method=\"pinv\", maxiter=2000)\n",
        "print(res.summary(yname=\"Y\", xname=[\"const\", \"Z\", \"U\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.997\n",
            "Model:                            OLS   Adj. R-squared:                  0.997\n",
            "Method:                 Least Squares   F-statistic:                 1.491e+06\n",
            "Date:                Thu, 11 Nov 2021   Prob (F-statistic):               0.00\n",
            "Time:                        15:22:10   Log-Likelihood:                 8662.7\n",
            "No. Observations:                9816   AIC:                        -1.732e+04\n",
            "Df Residuals:                    9813   BIC:                        -1.730e+04\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         -0.5011      0.001   -460.033      0.000      -0.503      -0.499\n",
            "Z              0.0501      0.000    248.215      0.000       0.050       0.050\n",
            "U              4.9971      0.003   1663.316      0.000       4.991       5.003\n",
            "==============================================================================\n",
            "Omnibus:                        0.071   Durbin-Watson:                   2.013\n",
            "Prob(Omnibus):                  0.965   Jarque-Bera (JB):                0.066\n",
            "Skew:                          -0.006   Prob(JB):                        0.968\n",
            "Kurtosis:                       3.002   Cond. No.                         15.3\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "CPU times: user 24.9 ms, sys: 12.7 ms, total: 37.6 ms\n",
            "Wall time: 25.8 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "girO2nFBsTjF"
      },
      "source": [
        "## Treatment Effect Estimation 2: Regress $Y_i$ on $Z_i$ only\n",
        "\n",
        "Since the estimation does not include the unobserved confounder $U_i$, the estimated treatment effect will\n",
        "be biased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B3gmvEFsTjF",
        "outputId": "859dbbda-285a-410e-909d-b4649db96d3b"
      },
      "source": [
        "%%time\n",
        "y = simulated_data[:, 0]\n",
        "X = sm.add_constant(simulated_data[:, 1:2])\n",
        "model = sm.OLS(endog=y, exog=X)\n",
        "res = model.fit(method=\"pinv\", maxiter=100)\n",
        "print(res.summary(yname=\"Y\", xname=[\"const\", \"Z\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      Y   R-squared:                       0.072\n",
            "Model:                            OLS   Adj. R-squared:                  0.072\n",
            "Method:                 Least Squares   F-statistic:                     763.9\n",
            "Date:                Thu, 11 Nov 2021   Prob (F-statistic):          5.45e-162\n",
            "Time:                        15:22:20   Log-Likelihood:                -19044.\n",
            "No. Observations:                9816   AIC:                         3.809e+04\n",
            "Df Residuals:                    9814   BIC:                         3.811e+04\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.1305      0.017      7.600      0.000       0.097       0.164\n",
            "Z              0.0930      0.003     27.639      0.000       0.086       0.100\n",
            "==============================================================================\n",
            "Omnibus:                     3666.367   Durbin-Watson:                   2.022\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             9959.843\n",
            "Skew:                           2.102   Prob(JB):                         0.00\n",
            "Kurtosis:                       5.585   Cond. No.                         5.16\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "CPU times: user 12.3 ms, sys: 5.07 ms, total: 17.3 ms\n",
            "Wall time: 17.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q-lDHTSsTjF"
      },
      "source": [
        "## Treatment Effect Estimation 3: Regress $Y_i$ on $Z_i$ and $\\pmb{X}_i$\n",
        "\n",
        "Do not run this, it takes too long, and may not even converge!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOCTGI95sTjG"
      },
      "source": [
        "# %%time\n",
        "# y = simulated_data[:, 0]\n",
        "# X = sm.add_constant(np.hstack((simulated_data[:, 1:2], simulated_data[:, 3:])))\n",
        "# model = sm.OLS(endog=y, exog=X)\n",
        "# res = model.fit(method=\"pinv\", maxiter=200)\n",
        "# res.summary(yname=\"Y\", xname=[\"const\", \"Z\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiwhno0csTjG"
      },
      "source": [
        "## Treatment Effect Estimation 4: Regress $Y_i$ on $Z_i$ and Document-Topic Weights\n",
        "\n",
        "Play around with the number of topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M2h4S2fsTjG",
        "outputId": "21c4349e-daf0-4f97-c759-a935d1896bc1"
      },
      "source": [
        "%%time\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "numtopics = 50\n",
        "nmf = NMF(n_components=numtopics).fit(tfidf_vectors)\n",
        "tfidf_feature_names = vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, 10)\n",
        "nmf_topic_weights = nmf.transform(tfidf_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #0: geb cadre dsl chastity n3jxp pitt intellect surrender shameful skepticism\n",
            "Topic #1: 3d object animation ray surface replies objects energy polygon libraries\n",
            "Topic #2: armenian armenians turkish armenia genocide russian soviet azerbaijan turkey turks\n",
            "Topic #3: motherboard cpu mhz fpu upgrade cache 386 slot processor nubus\n",
            "Topic #4: motif x11r5 xt toolkit interviews linux icon openwindows pixmap fixing\n",
            "Topic #5: pittsburgh detroit rangers leafs montreal islanders espn playoffs cup devils\n",
            "Topic #6: sin heaven spirit scripture mary eternal holy catholic matthew kingdom\n",
            "Topic #7: ide hd isa transfer bios sec boot mfm burst slave\n",
            "Topic #8: msg chinese foods eat reaction taste brain effects salt sick\n",
            "Topic #9: vga svga monitors lc modes compatible recommend adapter connect thanx\n",
            "Topic #10: moon launch orbit lunar shuttle mission satellite spacecraft solar mars\n",
            "Topic #11: objective morality subjective absolute animals goals relative observations immoral species\n",
            "Topic #12: motorcycle bikes ride dod riding helmet rider bmw dog motorcycles\n",
            "Topic #13: koresh batf compound waco cult warrant assault tear davidians branch\n",
            "Topic #14: font fonts characters xterm postscript character laser ps default printers\n",
            "Topic #15: arab arabs adam israelis lebanon lebanese palestinian palestinians attacks civilians\n",
            "Topic #16: keyboard shift focus typing 02 map characters patch xterm bios\n",
            "Topic #17: disease patients treatment diseases medicine cancer diagnosed patient syndrome diet\n",
            "Topic #18: audio circuit amp channel stereo voltage electronics speaker sony noise\n",
            "Topic #19: simms meg simm vram megs lc 4mb 72 quadra slots\n",
            "Topic #20: oil energy valve nuclear tube bolt cool heat temperature hole\n",
            "Topic #21: disks boot manuals install density offers docs formatted gateway swap\n",
            "Topic #22: amendment militia constitution firearms criminals weapon bear criminal rkba nra\n",
            "Topic #23: gif images bmp convert formats tiff jpeg bitmap shareware pictures\n",
            "Topic #24: pitching braves ball offense staff pitcher cubs hitter score alomar\n",
            "Topic #25: atheism atheists atheist beliefs gods universe bobby religions weak positive\n",
            "Topic #26: pin connector pins connect ports wire db dot 72 connected\n",
            "Topic #27: insurance rates accident coverage fault canadian vehicle lawyers germany paying\n",
            "Topic #28: ati vlb ultra gateway diamond isa eisa vesa s3 stealth\n",
            "Topic #29: drugs kids justify education social crack kid alcohol joke dealing\n",
            "Topic #30: gay homosexual sex sexual homosexuals homosexuality male marriage orientation married\n",
            "Topic #31: dealer prices saturn honda replaced sales quadra dealers profit mx\n",
            "Topic #32: islam cheers islamic kent muslims muslim religions notion teachings statements\n",
            "Topic #33: tax taxes income jobs bush congress sales economy funds deficit\n",
            "Topic #34: greek turkish turkey greece turks greeks cyprus island english minority\n",
            "Topic #35: widget null widgets button gl app int x11 xt xlib\n",
            "Topic #36: rear auto tires ford tank wheel tire toyota seat semi\n",
            "Topic #37: gm trial murray judge coach witnesses cup baltimore justice opening\n",
            "Topic #38: microsoft sys ini nt config exe bat backup norton swap\n",
            "Topic #39: radar detector ticket cop vehicle virginia photo antenna operating accurate\n",
            "Topic #40: __ ___ anonymous nist frame paint forever larry indicates music\n",
            "Topic #41: escrow enforcement privacy encrypted agencies warrant wiretap telephone cellular phones\n",
            "Topic #42: colors 256 vesa rgb colormap cells bytes colour 255 images\n",
            "Topic #43: nhl fans european stats canadian ice europe stars expansion finland\n",
            "Topic #44: battery batteries duo bmw sleep powerbook converter purchased ups backup\n",
            "Topic #45: nsa des crypto rsa bits pgp cryptography crypt classified random\n",
            "Topic #46: zip cica shareware bbs indiana win3 download sites desktop msdos\n",
            "Topic #47: doctor pain poster brain treatment patient surgery doctors flame arm\n",
            "Topic #48: mailing lists subscribe addresses junk frank catalog scores instructions join\n",
            "Topic #49: tek vice sea blew xterm terminal sig frank versa ticket\n",
            "\n",
            "CPU times: user 1min 54s, sys: 9.92 s, total: 2min 4s\n",
            "Wall time: 1min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm4EZAQzsTjG",
        "outputId": "52cc9094-dac8-4605-8217-e51f692d2598"
      },
      "source": [
        "%%time\n",
        "y = simulated_data[:, 0]\n",
        "X = sm.add_constant(np.hstack((simulated_data[:, 1:2], nmf_topic_weights)))\n",
        "model = sm.OLS(endog=y, exog=X)\n",
        "res = model.fit(method=\"pinv\", maxiter=200)\n",
        "print(res.summary(xname=[\"const\", \"Z\"] + [\"Topic\" + str(i+1) for i in range(numtopics)]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.533\n",
            "Model:                            OLS   Adj. R-squared:                  0.531\n",
            "Method:                 Least Squares   F-statistic:                     218.6\n",
            "Date:                Thu, 11 Nov 2021   Prob (F-statistic):               0.00\n",
            "Time:                        15:24:27   Log-Likelihood:                -15674.\n",
            "No. Observations:                9816   AIC:                         3.145e+04\n",
            "Df Residuals:                    9764   BIC:                         3.183e+04\n",
            "Df Model:                          51                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.0730      0.029      2.512      0.012       0.016       0.130\n",
            "Z              0.0704      0.002     29.188      0.000       0.066       0.075\n",
            "Topic1        -1.9513      0.488     -4.001      0.000      -2.907      -0.995\n",
            "Topic2       -14.6164      4.071     -3.590      0.000     -22.597      -6.636\n",
            "Topic3        -3.9284      0.702     -5.600      0.000      -5.303      -2.553\n",
            "Topic4        -5.7560      1.031     -5.583      0.000      -7.777      -3.735\n",
            "Topic5        -3.4319      1.160     -2.959      0.003      -5.705      -1.159\n",
            "Topic6       -12.5585      1.666     -7.537      0.000     -15.825      -9.292\n",
            "Topic7        64.0880      1.015     63.111      0.000      62.097      66.079\n",
            "Topic8        -5.8941      1.772     -3.327      0.001      -9.367      -2.421\n",
            "Topic9        -2.6486      0.746     -3.551      0.000      -4.111      -1.186\n",
            "Topic10       -4.5788      1.831     -2.500      0.012      -8.169      -0.989\n",
            "Topic11       -7.4207      1.037     -7.154      0.000      -9.454      -5.387\n",
            "Topic12       33.7486      1.369     24.648      0.000      31.065      36.433\n",
            "Topic13       -5.4277      0.890     -6.102      0.000      -7.171      -3.684\n",
            "Topic14       13.9558      1.143     12.208      0.000      11.715      16.197\n",
            "Topic15       -6.5050      1.646     -3.951      0.000      -9.732      -3.278\n",
            "Topic16       -4.6133      0.763     -6.046      0.000      -6.109      -3.118\n",
            "Topic17       -6.7422      2.295     -2.938      0.003     -11.241      -2.243\n",
            "Topic18       -3.1504      1.052     -2.996      0.003      -5.212      -1.089\n",
            "Topic19      -13.9050      1.836     -7.575      0.000     -17.503     -10.307\n",
            "Topic20       -3.0120      0.956     -3.149      0.002      -4.887      -1.137\n",
            "Topic21       -2.3001      1.101     -2.088      0.037      -4.459      -0.141\n",
            "Topic22       -4.2985      1.310     -3.282      0.001      -6.866      -1.731\n",
            "Topic23       -7.9556      1.761     -4.518      0.000     -11.407      -4.504\n",
            "Topic24       -4.8242      1.195     -4.035      0.000      -7.168      -2.481\n",
            "Topic25       -7.0032      0.900     -7.780      0.000      -8.768      -5.239\n",
            "Topic26       43.6702      1.298     33.656      0.000      41.127      46.214\n",
            "Topic27       -4.3199      1.168     -3.698      0.000      -6.610      -2.030\n",
            "Topic28       -1.0381      0.881     -1.178      0.239      -2.765       0.689\n",
            "Topic29       -3.5564      1.026     -3.465      0.001      -5.568      -1.545\n",
            "Topic30       -0.8076      1.430     -0.565      0.572      -3.610       1.995\n",
            "Topic31       12.0441      1.171     10.287      0.000       9.749      14.339\n",
            "Topic32       -3.2074      0.870     -3.685      0.000      -4.914      -1.501\n",
            "Topic33       26.7212      1.113     24.013      0.000      24.540      28.902\n",
            "Topic34       -3.0604      0.767     -3.988      0.000      -4.565      -1.556\n",
            "Topic35        1.1269      1.016      1.109      0.267      -0.865       3.119\n",
            "Topic36       -5.4264      1.191     -4.556      0.000      -7.761      -3.092\n",
            "Topic37       -8.3514      1.311     -6.368      0.000     -10.922      -5.781\n",
            "Topic38       -3.2931      1.054     -3.124      0.002      -5.359      -1.227\n",
            "Topic39       -4.8702      0.951     -5.119      0.000      -6.735      -3.005\n",
            "Topic40       -4.2143      1.240     -3.399      0.001      -6.645      -1.784\n",
            "Topic41       -1.7822      0.849     -2.099      0.036      -3.447      -0.118\n",
            "Topic42       -4.5879      0.841     -5.457      0.000      -6.236      -2.940\n",
            "Topic43       -2.6594      0.887     -2.999      0.003      -4.398      -0.921\n",
            "Topic44       -2.5643      0.739     -3.471      0.001      -4.012      -1.116\n",
            "Topic45       -2.7156      0.940     -2.890      0.004      -4.557      -0.874\n",
            "Topic46       -3.3538      0.738     -4.542      0.000      -4.801      -1.906\n",
            "Topic47       -2.5038      0.751     -3.332      0.001      -3.977      -1.031\n",
            "Topic48       -1.2608      0.687     -1.835      0.066      -2.607       0.086\n",
            "Topic49       -0.4895      0.680     -0.720      0.471      -1.822       0.843\n",
            "Topic50        7.8604      0.684     11.494      0.000       6.520       9.201\n",
            "==============================================================================\n",
            "Omnibus:                     3687.833   Durbin-Watson:                   1.997\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16012.870\n",
            "Skew:                           1.815   Prob(JB):                         0.00\n",
            "Kurtosis:                       8.097   Cond. No.                     1.73e+03\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.73e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "CPU times: user 133 ms, sys: 45.4 ms, total: 178 ms\n",
            "Wall time: 98.6 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS8HRCdgsTjH"
      },
      "source": [
        "## Treatment Effect Estimation 5: Regress $Y_i$ on $Z_i$ and $\\pmb{X}_i$ using Double Machine Learning\n",
        "\n",
        "Play around with the type of ML models used to predict the treatment and outcome.\n",
        "\n",
        "Note that some models take really long to train (eg. Random Forests)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE0qMr9rsTjH"
      },
      "source": [
        "### Using the [EconML](https://econml.azurewebsites.net/) package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb7ic4wcsTjH",
        "outputId": "5ba7d77a-babc-4702-d90a-518959559cb8"
      },
      "source": [
        "%%time\n",
        "Y = simulated_data[:, 0].ravel() # outcome\n",
        "T = simulated_data[:, 1] # treatment\n",
        "W = simulated_data[:, 3:] # text\n",
        "\n",
        "dml_mult = LinearDMLCateEstimator(model_y=LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1),\n",
        "                                  model_t=LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1),\n",
        "                                  linear_first_stages=True, n_splits=2)\n",
        "\n",
        "dml_mult.fit(Y=Y, T=T, W=W, inference=\"statsmodels\")\n",
        "te_pred_mult = dml_mult.const_marginal_effect().ravel()\n",
        "\n",
        "lb_mult, ub_mult = dml_mult.const_marginal_effect_interval(alpha=0.05)\n",
        "lb_mult, ub_mult = lb_mult.ravel(), ub_mult.ravel()\n",
        "\n",
        "print(\"Treatment Effect, 95% CI\" + \"\\t\\t\" + \"{:.6f}\".format(te_pred_mult[0]) +\\\n",
        "      \"({:.6f}\".format(lb_mult[0]) + \", \" + \"{:.6f})\".format(ub_mult[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treatment Effect, 95% CI\t\t0.050828(0.047201, 0.054456)\n",
            "CPU times: user 3min 52s, sys: 10.7 s, total: 4min 3s\n",
            "Wall time: 2min 39s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vGRwqi2sTjH"
      },
      "source": [
        "### Using the [DoubleML](https://docs.doubleml.org/stable/index.html) package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1dLBQIIsTjI",
        "outputId": "a2545d82-04e7-4eae-c74d-c7c4f18b64db"
      },
      "source": [
        "%%time\n",
        "data = doubleml.DoubleMLData(data=df, y_col=\"Y\", d_cols=\"Z\",\n",
        "                             x_cols=[\"Word\" + str(i) for i in range(len(df.columns)-3)])\n",
        "\n",
        "treatment_predictor = LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1)\n",
        "outcome_predictor = LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1)\n",
        "\n",
        "estimator = doubleml.DoubleMLPLR(data, treatment_predictor, outcome_predictor,\n",
        "                                 apply_cross_fitting=False, n_folds=2)\n",
        "estimator.fit()\n",
        "print(estimator.summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       coef   std err          t         P>|t|     2.5 %    97.5 %\n",
            "Z  0.040553  0.002515  16.123596  1.741738e-58  0.035623  0.045482\n",
            "CPU times: user 1min 6s, sys: 1.36 s, total: 1min 8s\n",
            "Wall time: 1min 49s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqycQtI4sTjI"
      },
      "source": [
        "### From Scratch\n",
        "\n",
        "Note: Standard errors are different from those recommended in the double machine learning paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdgeMLy4sTjI",
        "outputId": "654cd066-44b0-4470-fe7d-0c153a140d77"
      },
      "source": [
        "%%time\n",
        "simulated_data_train, simulated_data_inference = train_test_split(simulated_data, test_size=0.5)\n",
        "\n",
        "Y_train = simulated_data_train[:, 0]\n",
        "T_train = simulated_data_train[:, 1]\n",
        "text_train = simulated_data_train[:, 3:]\n",
        "\n",
        "Y_inference = simulated_data_inference[:, 0]\n",
        "T_inference = simulated_data_inference[:, 1]\n",
        "text_inference = simulated_data_inference[:, 3:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 104 ms, sys: 1.87 ms, total: 106 ms\n",
            "Wall time: 111 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AJWPOpxsTjI",
        "outputId": "20b731b1-59ba-4720-f519-57f8f9dc9187"
      },
      "source": [
        "%%time\n",
        "model_y_tr = LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1).fit(X=text_train, y=Y_train)\n",
        "model_t_tr = LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1).fit(X=text_train, y=T_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 13s, sys: 10.3 s, total: 2min 24s\n",
            "Wall time: 1min 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6ZPx63vsTjJ",
        "outputId": "8b29501a-8ed9-4c92-ef94-794f784f6634"
      },
      "source": [
        "%%time\n",
        "model_y_inf = LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1).fit(X=text_inference, y=Y_inference)\n",
        "model_t_inf = LassoCV(cv=2, n_alphas=1, verbose=0, n_jobs=-1).fit(X=text_inference, y=T_inference)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 7s, sys: 7.71 s, total: 2min 15s\n",
            "Wall time: 1min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJlqr_iKsTjJ"
      },
      "source": [
        "yres1 = Y_inference - model_y_tr.predict(text_inference)\n",
        "tres1 = T_inference - model_t_tr.predict(text_inference)\n",
        "yres2 = Y_train - model_y_inf.predict(text_train)\n",
        "tres2 = T_train - model_t_inf.predict(text_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhNzaFsVsTjJ",
        "outputId": "b65848d1-46f6-4546-82c6-96a39d4b3805"
      },
      "source": [
        "theta_1 = np.mean(yres1*tres1)/np.mean(tres1**2)\n",
        "theta_2 = np.mean(yres2*tres2)/np.mean(tres2**2)\n",
        "theta = 0.5 * (theta_1 + theta_2)\n",
        "print(\"Cross-Fitted Treatment Effect\", theta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Fitted Treatment Effect 0.05048492653618829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEcgLdFIsTjJ",
        "outputId": "3e76481c-c854-4c12-f2a3-21e0e58371fd"
      },
      "source": [
        "print(\"Sample-Split Treatment Effect\")\n",
        "regression_model = sm.OLS(endog=yres1, exog=tres1, hasconst=False)\n",
        "res = regression_model.fit(method=\"pinv\", maxiter=100)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample-Split Treatment Effect\n",
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                      y   R-squared (uncentered):                   0.069\n",
            "Model:                            OLS   Adj. R-squared (uncentered):              0.069\n",
            "Method:                 Least Squares   F-statistic:                              363.8\n",
            "Date:                Thu, 11 Nov 2021   Prob (F-statistic):                    2.67e-78\n",
            "Time:                        15:33:04   Log-Likelihood:                         -10071.\n",
            "No. Observations:                4908   AIC:                                  2.014e+04\n",
            "Df Residuals:                    4907   BIC:                                  2.015e+04\n",
            "Df Model:                           1                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "x1             0.0469      0.002     19.074      0.000       0.042       0.052\n",
            "==============================================================================\n",
            "Omnibus:                      293.163   Durbin-Watson:                   1.960\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              714.271\n",
            "Skew:                           0.357   Prob(JB):                    7.91e-156\n",
            "Kurtosis:                       4.727   Cond. No.                         1.00\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96UkPQx1nulr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}